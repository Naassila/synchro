{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synchro Project\n",
    "- [github link](https://github.com/romainmartinez/envergo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Figures\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "sns.set_context(\"notebook\", font_scale=1.1)\n",
    "sns.set_style(\"ticks\")\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from mat\n",
    "DATA_PATH = './data/'\n",
    "X = sio.loadmat(os.path.join(DATA_PATH, 'X.mat'))['TableauForces']\n",
    "y = sio.loadmat(os.path.join(DATA_PATH, 'y.mat'))['TestData']\n",
    "\n",
    "X_description = np.array(['AddL', 'AddR', 'AbdL', 'AbdR', 'ErL', 'ErR', 'IrL', 'IrR', 'ExtL', 'ExtR', 'FlexL', 'FlexR'])\n",
    "y_description = np.array(['Dyn', 'BodyBoost', 'MeanEggBeater', 'MaxEggBeater'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 12)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nan remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tparticipant n: 1\n",
      "\ttest: IrL\n",
      "\t\t\"nan\" replace by \"118.95\"\n",
      "\t -----\n",
      "\tparticipant n: 51\n",
      "\ttest: IrL\n",
      "\t\t\"nan\" replace by \"92.25\"\n",
      "\t -----\n"
     ]
    }
   ],
   "source": [
    "nan_id = np.argwhere(np.isnan(X))\n",
    "n_nans = np.sum(np.isnan(X).sum(axis=1))\n",
    "for i in nan_id:\n",
    "    print(f'\\tparticipant n: {i[0]}')\n",
    "    print(f'\\ttest: {X_description[i[1]]}')\n",
    "    # if left take right, left otherwise\n",
    "    if X_description[i[1]][-1] == 'L':\n",
    "        replacer = i[1] + 1\n",
    "    elif X_description[i[1]][-1] == 'R':\n",
    "        replacer = i[1] - 1\n",
    "    print(f'\\t\\t\"{X[i[0], i[1]]}\" replace by \"{X[i[0], replacer]}\"')\n",
    "    X[i[0], i[1]] = X[i[0], replacer]\n",
    "    print('\\t', '-' * 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load height + weight\n",
    "anthropo = sio.loadmat(os.path.join(DATA_PATH, 'heightweight.mat'))['HeightWeight']\n",
    "# replace nan\n",
    "from sklearn.preprocessing import Imputer\n",
    "anthropo = Imputer(strategy='median').fit_transform(anthropo)\n",
    "# add IMC\n",
    "anthropo = np.c_[anthropo, anthropo[:, 1] / (anthropo[:, 0])**2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute imbalance\n",
    "imbalance = None\n",
    "for i in range(0, X.shape[1], 2):\n",
    "    if imbalance is None:\n",
    "        imbalance = np.abs((X[:, i] - X[:, i + 1]) / X[:, i]) * 100\n",
    "    else:\n",
    "        imbalance = np.c_[imbalance, np.abs((X[:, i] - X[:, i + 1]) / X[:, i]) * 100]\n",
    "imbalance = np.mean(imbalance, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[X, anthropo, imbalance]\n",
    "X_description = np.append(X_description, ['height', 'weight', 'IMC', 'imbalance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cols = {\n",
    "    'test': np.arange(12),\n",
    "    'anthropo': np.arange(12, 15),\n",
    "    'imbalance': np.array([15])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom class\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class Normalize(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Normalize a given array with weight, height or IMC\"\"\"\n",
    "    def __init__(self, X_cols=X_cols, strategy='IMC'):\n",
    "        self.strategy = strategy\n",
    "        self.X_cols = X_cols\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        if self.strategy == 'height':\n",
    "            normalizer = X_copy[:, X_cols['anthropo']][:, 0].reshape(-1, 1)\n",
    "        elif self.strategy == 'weight':\n",
    "            normalizer = X_copy[:, X_cols['anthropo']][:, 1].reshape(-1, 1)\n",
    "        elif self.strategy == 'IMC':\n",
    "            normalizer = X_copy[:, X_cols['anthropo']][:, 2].reshape(-1, 1)\n",
    "        else:\n",
    "            normalizer = 1\n",
    "        X_copy[:, X_cols['test']] = X_copy[:, X_cols['test']] / normalizer\n",
    "        self.output = X_copy\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.output\n",
    "    \n",
    "class TestSide(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Return the mean between left & right or both\"\"\"\n",
    "    def __init__(self, X_cols=X_cols, strategy='mean'):\n",
    "        self.strategy = strategy\n",
    "        self.X_cols = X_cols\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        if self.strategy == 'mean':\n",
    "            output = None\n",
    "            for i in range(0, X_cols['test'][-1] + 1, 2):\n",
    "                if output is None:\n",
    "                    output = np.mean([X[:, i], X[:, i + 1]], axis=0)\n",
    "                else:\n",
    "                    output = np.c_[output, np.mean([X[:, i], X[:, i + 1]], axis=0)]\n",
    "        else:\n",
    "            output = X\n",
    "        self.output = output\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.output\n",
    "    \n",
    "class FeaturesAdder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Add features based on the list `new_features`\n",
    "    Possible `new_features` are: IMC, imbalance\n",
    "    \"\"\"\n",
    "    def __init__(self, X_cols=X_cols, new_features='None'):\n",
    "        self.new_features = new_features\n",
    "        self.X_cols = X_cols\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        X_copy = X[:, X_cols['test']].copy()\n",
    "        if 'IMC' in self.new_features:\n",
    "            X_copy = np.c_[X_copy, X[:, X_cols['anthropo']][:, 2]]\n",
    "        if 'imbalance' in self.new_features:\n",
    "            X_copy = np.c_[X_copy, X[:, X_cols['imbalance']]]\n",
    "        if 'height-weight' in self.new_features:\n",
    "            X_copy = np.c_[X_copy, X[:, X_cols['anthropo']][:, 0:2]]\n",
    "        self.output = X_copy\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230.25</td>\n",
       "      <td>247.50</td>\n",
       "      <td>269.75</td>\n",
       "      <td>262.05</td>\n",
       "      <td>96.65</td>\n",
       "      <td>103.50</td>\n",
       "      <td>124.95</td>\n",
       "      <td>121.90</td>\n",
       "      <td>213.50</td>\n",
       "      <td>175.65</td>\n",
       "      <td>184.60</td>\n",
       "      <td>208.25</td>\n",
       "      <td>22.119015</td>\n",
       "      <td>8.402430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>283.40</td>\n",
       "      <td>289.85</td>\n",
       "      <td>251.85</td>\n",
       "      <td>263.45</td>\n",
       "      <td>88.90</td>\n",
       "      <td>87.05</td>\n",
       "      <td>100.80</td>\n",
       "      <td>88.80</td>\n",
       "      <td>224.65</td>\n",
       "      <td>274.90</td>\n",
       "      <td>156.85</td>\n",
       "      <td>162.50</td>\n",
       "      <td>22.959088</td>\n",
       "      <td>7.806316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>261.55</td>\n",
       "      <td>255.95</td>\n",
       "      <td>213.65</td>\n",
       "      <td>207.60</td>\n",
       "      <td>75.70</td>\n",
       "      <td>80.70</td>\n",
       "      <td>104.70</td>\n",
       "      <td>80.55</td>\n",
       "      <td>118.65</td>\n",
       "      <td>97.90</td>\n",
       "      <td>177.80</td>\n",
       "      <td>155.50</td>\n",
       "      <td>20.443594</td>\n",
       "      <td>10.779055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>256.15</td>\n",
       "      <td>275.05</td>\n",
       "      <td>238.15</td>\n",
       "      <td>236.10</td>\n",
       "      <td>61.50</td>\n",
       "      <td>73.15</td>\n",
       "      <td>105.55</td>\n",
       "      <td>87.90</td>\n",
       "      <td>173.55</td>\n",
       "      <td>190.45</td>\n",
       "      <td>156.30</td>\n",
       "      <td>148.60</td>\n",
       "      <td>18.710698</td>\n",
       "      <td>9.761427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>193.20</td>\n",
       "      <td>208.70</td>\n",
       "      <td>190.50</td>\n",
       "      <td>162.30</td>\n",
       "      <td>81.65</td>\n",
       "      <td>84.40</td>\n",
       "      <td>93.25</td>\n",
       "      <td>79.00</td>\n",
       "      <td>119.95</td>\n",
       "      <td>107.55</td>\n",
       "      <td>108.45</td>\n",
       "      <td>126.30</td>\n",
       "      <td>19.066406</td>\n",
       "      <td>11.378716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>236.05</td>\n",
       "      <td>247.60</td>\n",
       "      <td>250.60</td>\n",
       "      <td>264.65</td>\n",
       "      <td>73.50</td>\n",
       "      <td>79.95</td>\n",
       "      <td>101.65</td>\n",
       "      <td>109.00</td>\n",
       "      <td>257.45</td>\n",
       "      <td>250.35</td>\n",
       "      <td>193.70</td>\n",
       "      <td>186.45</td>\n",
       "      <td>21.171885</td>\n",
       "      <td>5.501083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>271.10</td>\n",
       "      <td>280.30</td>\n",
       "      <td>194.50</td>\n",
       "      <td>199.30</td>\n",
       "      <td>97.30</td>\n",
       "      <td>100.10</td>\n",
       "      <td>142.70</td>\n",
       "      <td>129.05</td>\n",
       "      <td>128.80</td>\n",
       "      <td>137.40</td>\n",
       "      <td>176.35</td>\n",
       "      <td>177.35</td>\n",
       "      <td>18.339100</td>\n",
       "      <td>4.258123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>316.40</td>\n",
       "      <td>317.90</td>\n",
       "      <td>222.25</td>\n",
       "      <td>226.80</td>\n",
       "      <td>99.75</td>\n",
       "      <td>117.55</td>\n",
       "      <td>134.75</td>\n",
       "      <td>134.80</td>\n",
       "      <td>121.35</td>\n",
       "      <td>222.70</td>\n",
       "      <td>237.55</td>\n",
       "      <td>219.20</td>\n",
       "      <td>23.323416</td>\n",
       "      <td>18.607747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>262.40</td>\n",
       "      <td>256.50</td>\n",
       "      <td>191.40</td>\n",
       "      <td>191.85</td>\n",
       "      <td>99.45</td>\n",
       "      <td>111.35</td>\n",
       "      <td>174.75</td>\n",
       "      <td>155.85</td>\n",
       "      <td>113.90</td>\n",
       "      <td>151.70</td>\n",
       "      <td>148.35</td>\n",
       "      <td>119.65</td>\n",
       "      <td>18.903592</td>\n",
       "      <td>12.966332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>239.40</td>\n",
       "      <td>272.40</td>\n",
       "      <td>239.80</td>\n",
       "      <td>242.25</td>\n",
       "      <td>79.95</td>\n",
       "      <td>95.60</td>\n",
       "      <td>120.95</td>\n",
       "      <td>108.95</td>\n",
       "      <td>150.25</td>\n",
       "      <td>154.80</td>\n",
       "      <td>185.70</td>\n",
       "      <td>176.15</td>\n",
       "      <td>20.588235</td>\n",
       "      <td>8.745554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>257.20</td>\n",
       "      <td>266.80</td>\n",
       "      <td>270.15</td>\n",
       "      <td>259.20</td>\n",
       "      <td>110.35</td>\n",
       "      <td>108.00</td>\n",
       "      <td>122.10</td>\n",
       "      <td>118.70</td>\n",
       "      <td>228.90</td>\n",
       "      <td>261.30</td>\n",
       "      <td>203.70</td>\n",
       "      <td>173.35</td>\n",
       "      <td>21.110727</td>\n",
       "      <td>6.959002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>117.75</td>\n",
       "      <td>126.15</td>\n",
       "      <td>135.90</td>\n",
       "      <td>129.65</td>\n",
       "      <td>46.80</td>\n",
       "      <td>57.30</td>\n",
       "      <td>95.20</td>\n",
       "      <td>66.85</td>\n",
       "      <td>73.60</td>\n",
       "      <td>91.30</td>\n",
       "      <td>134.25</td>\n",
       "      <td>134.40</td>\n",
       "      <td>17.977701</td>\n",
       "      <td>14.684780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>259.15</td>\n",
       "      <td>257.30</td>\n",
       "      <td>221.15</td>\n",
       "      <td>210.75</td>\n",
       "      <td>55.45</td>\n",
       "      <td>54.25</td>\n",
       "      <td>61.40</td>\n",
       "      <td>60.05</td>\n",
       "      <td>143.85</td>\n",
       "      <td>89.25</td>\n",
       "      <td>138.65</td>\n",
       "      <td>132.05</td>\n",
       "      <td>20.746579</td>\n",
       "      <td>8.749294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>243.25</td>\n",
       "      <td>251.40</td>\n",
       "      <td>243.45</td>\n",
       "      <td>246.60</td>\n",
       "      <td>73.15</td>\n",
       "      <td>80.10</td>\n",
       "      <td>112.65</td>\n",
       "      <td>110.10</td>\n",
       "      <td>106.75</td>\n",
       "      <td>135.20</td>\n",
       "      <td>122.10</td>\n",
       "      <td>135.05</td>\n",
       "      <td>21.303949</td>\n",
       "      <td>8.944358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>235.80</td>\n",
       "      <td>258.95</td>\n",
       "      <td>165.50</td>\n",
       "      <td>171.50</td>\n",
       "      <td>76.95</td>\n",
       "      <td>85.05</td>\n",
       "      <td>92.35</td>\n",
       "      <td>104.50</td>\n",
       "      <td>106.75</td>\n",
       "      <td>116.45</td>\n",
       "      <td>124.95</td>\n",
       "      <td>124.10</td>\n",
       "      <td>17.716263</td>\n",
       "      <td>7.815455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>284.40</td>\n",
       "      <td>252.40</td>\n",
       "      <td>240.40</td>\n",
       "      <td>218.30</td>\n",
       "      <td>87.50</td>\n",
       "      <td>84.55</td>\n",
       "      <td>92.45</td>\n",
       "      <td>106.10</td>\n",
       "      <td>160.10</td>\n",
       "      <td>177.20</td>\n",
       "      <td>152.35</td>\n",
       "      <td>157.85</td>\n",
       "      <td>19.571681</td>\n",
       "      <td>8.811978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>202.45</td>\n",
       "      <td>210.55</td>\n",
       "      <td>231.75</td>\n",
       "      <td>238.35</td>\n",
       "      <td>84.15</td>\n",
       "      <td>77.20</td>\n",
       "      <td>96.10</td>\n",
       "      <td>90.15</td>\n",
       "      <td>146.35</td>\n",
       "      <td>176.75</td>\n",
       "      <td>143.45</td>\n",
       "      <td>132.70</td>\n",
       "      <td>19.694689</td>\n",
       "      <td>8.260906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>211.25</td>\n",
       "      <td>215.30</td>\n",
       "      <td>192.60</td>\n",
       "      <td>183.90</td>\n",
       "      <td>76.90</td>\n",
       "      <td>83.45</td>\n",
       "      <td>54.75</td>\n",
       "      <td>67.80</td>\n",
       "      <td>132.20</td>\n",
       "      <td>183.70</td>\n",
       "      <td>172.80</td>\n",
       "      <td>199.25</td>\n",
       "      <td>22.392290</td>\n",
       "      <td>15.508384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>262.05</td>\n",
       "      <td>269.00</td>\n",
       "      <td>215.15</td>\n",
       "      <td>187.30</td>\n",
       "      <td>82.45</td>\n",
       "      <td>86.65</td>\n",
       "      <td>128.15</td>\n",
       "      <td>116.60</td>\n",
       "      <td>176.65</td>\n",
       "      <td>168.45</td>\n",
       "      <td>185.65</td>\n",
       "      <td>176.85</td>\n",
       "      <td>20.402309</td>\n",
       "      <td>6.514257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>231.25</td>\n",
       "      <td>253.35</td>\n",
       "      <td>276.05</td>\n",
       "      <td>253.95</td>\n",
       "      <td>71.55</td>\n",
       "      <td>79.75</td>\n",
       "      <td>86.95</td>\n",
       "      <td>90.25</td>\n",
       "      <td>200.10</td>\n",
       "      <td>145.05</td>\n",
       "      <td>172.50</td>\n",
       "      <td>167.45</td>\n",
       "      <td>21.714286</td>\n",
       "      <td>10.542856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>220.95</td>\n",
       "      <td>227.05</td>\n",
       "      <td>185.15</td>\n",
       "      <td>194.90</td>\n",
       "      <td>45.10</td>\n",
       "      <td>58.75</td>\n",
       "      <td>103.25</td>\n",
       "      <td>117.55</td>\n",
       "      <td>124.10</td>\n",
       "      <td>174.80</td>\n",
       "      <td>122.75</td>\n",
       "      <td>109.95</td>\n",
       "      <td>20.237387</td>\n",
       "      <td>17.237435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>232.85</td>\n",
       "      <td>246.70</td>\n",
       "      <td>193.30</td>\n",
       "      <td>187.70</td>\n",
       "      <td>81.80</td>\n",
       "      <td>87.00</td>\n",
       "      <td>103.35</td>\n",
       "      <td>96.20</td>\n",
       "      <td>136.30</td>\n",
       "      <td>117.75</td>\n",
       "      <td>161.00</td>\n",
       "      <td>154.80</td>\n",
       "      <td>19.759871</td>\n",
       "      <td>6.596818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>272.40</td>\n",
       "      <td>273.95</td>\n",
       "      <td>224.05</td>\n",
       "      <td>229.75</td>\n",
       "      <td>87.65</td>\n",
       "      <td>90.95</td>\n",
       "      <td>88.20</td>\n",
       "      <td>98.25</td>\n",
       "      <td>187.65</td>\n",
       "      <td>131.85</td>\n",
       "      <td>141.60</td>\n",
       "      <td>172.55</td>\n",
       "      <td>20.807487</td>\n",
       "      <td>11.644363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>181.75</td>\n",
       "      <td>191.50</td>\n",
       "      <td>204.05</td>\n",
       "      <td>209.95</td>\n",
       "      <td>67.90</td>\n",
       "      <td>85.05</td>\n",
       "      <td>75.55</td>\n",
       "      <td>73.75</td>\n",
       "      <td>91.85</td>\n",
       "      <td>69.40</td>\n",
       "      <td>155.80</td>\n",
       "      <td>168.55</td>\n",
       "      <td>21.679401</td>\n",
       "      <td>11.420302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>301.25</td>\n",
       "      <td>298.05</td>\n",
       "      <td>272.55</td>\n",
       "      <td>292.15</td>\n",
       "      <td>96.80</td>\n",
       "      <td>100.40</td>\n",
       "      <td>118.95</td>\n",
       "      <td>118.95</td>\n",
       "      <td>165.05</td>\n",
       "      <td>159.30</td>\n",
       "      <td>222.25</td>\n",
       "      <td>213.80</td>\n",
       "      <td>22.862369</td>\n",
       "      <td>3.209735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>199.85</td>\n",
       "      <td>223.15</td>\n",
       "      <td>226.00</td>\n",
       "      <td>217.20</td>\n",
       "      <td>83.40</td>\n",
       "      <td>89.00</td>\n",
       "      <td>112.60</td>\n",
       "      <td>93.20</td>\n",
       "      <td>214.30</td>\n",
       "      <td>167.70</td>\n",
       "      <td>177.80</td>\n",
       "      <td>166.95</td>\n",
       "      <td>20.826921</td>\n",
       "      <td>11.223981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>202.05</td>\n",
       "      <td>219.00</td>\n",
       "      <td>160.60</td>\n",
       "      <td>163.00</td>\n",
       "      <td>62.50</td>\n",
       "      <td>78.60</td>\n",
       "      <td>94.60</td>\n",
       "      <td>90.80</td>\n",
       "      <td>196.85</td>\n",
       "      <td>171.25</td>\n",
       "      <td>127.10</td>\n",
       "      <td>117.35</td>\n",
       "      <td>19.136719</td>\n",
       "      <td>10.056046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>199.60</td>\n",
       "      <td>199.80</td>\n",
       "      <td>231.50</td>\n",
       "      <td>218.45</td>\n",
       "      <td>61.40</td>\n",
       "      <td>73.55</td>\n",
       "      <td>87.15</td>\n",
       "      <td>90.85</td>\n",
       "      <td>138.45</td>\n",
       "      <td>178.70</td>\n",
       "      <td>119.80</td>\n",
       "      <td>115.90</td>\n",
       "      <td>21.852535</td>\n",
       "      <td>10.349745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>280.55</td>\n",
       "      <td>273.80</td>\n",
       "      <td>212.35</td>\n",
       "      <td>194.55</td>\n",
       "      <td>77.40</td>\n",
       "      <td>78.40</td>\n",
       "      <td>92.50</td>\n",
       "      <td>78.10</td>\n",
       "      <td>213.15</td>\n",
       "      <td>197.85</td>\n",
       "      <td>230.90</td>\n",
       "      <td>232.80</td>\n",
       "      <td>19.930796</td>\n",
       "      <td>5.941474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>227.85</td>\n",
       "      <td>228.30</td>\n",
       "      <td>178.80</td>\n",
       "      <td>163.80</td>\n",
       "      <td>77.10</td>\n",
       "      <td>74.15</td>\n",
       "      <td>106.45</td>\n",
       "      <td>102.10</td>\n",
       "      <td>260.65</td>\n",
       "      <td>202.15</td>\n",
       "      <td>180.00</td>\n",
       "      <td>171.70</td>\n",
       "      <td>21.250983</td>\n",
       "      <td>7.259064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>282.05</td>\n",
       "      <td>252.40</td>\n",
       "      <td>239.25</td>\n",
       "      <td>225.30</td>\n",
       "      <td>82.20</td>\n",
       "      <td>91.70</td>\n",
       "      <td>79.70</td>\n",
       "      <td>87.50</td>\n",
       "      <td>106.55</td>\n",
       "      <td>73.90</td>\n",
       "      <td>153.15</td>\n",
       "      <td>140.95</td>\n",
       "      <td>20.485977</td>\n",
       "      <td>12.715976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>319.65</td>\n",
       "      <td>333.95</td>\n",
       "      <td>260.70</td>\n",
       "      <td>261.25</td>\n",
       "      <td>74.65</td>\n",
       "      <td>91.90</td>\n",
       "      <td>111.50</td>\n",
       "      <td>104.20</td>\n",
       "      <td>241.00</td>\n",
       "      <td>237.15</td>\n",
       "      <td>202.55</td>\n",
       "      <td>185.35</td>\n",
       "      <td>21.420747</td>\n",
       "      <td>7.404796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>331.55</td>\n",
       "      <td>322.15</td>\n",
       "      <td>211.05</td>\n",
       "      <td>235.40</td>\n",
       "      <td>82.70</td>\n",
       "      <td>93.50</td>\n",
       "      <td>161.95</td>\n",
       "      <td>145.95</td>\n",
       "      <td>182.40</td>\n",
       "      <td>204.95</td>\n",
       "      <td>197.55</td>\n",
       "      <td>218.25</td>\n",
       "      <td>22.589532</td>\n",
       "      <td>10.025477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>211.00</td>\n",
       "      <td>216.40</td>\n",
       "      <td>171.55</td>\n",
       "      <td>160.70</td>\n",
       "      <td>67.90</td>\n",
       "      <td>72.55</td>\n",
       "      <td>78.20</td>\n",
       "      <td>69.15</td>\n",
       "      <td>110.40</td>\n",
       "      <td>90.10</td>\n",
       "      <td>136.60</td>\n",
       "      <td>132.05</td>\n",
       "      <td>18.749258</td>\n",
       "      <td>8.170617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>263.40</td>\n",
       "      <td>274.00</td>\n",
       "      <td>237.55</td>\n",
       "      <td>209.95</td>\n",
       "      <td>63.70</td>\n",
       "      <td>66.05</td>\n",
       "      <td>85.70</td>\n",
       "      <td>92.30</td>\n",
       "      <td>300.20</td>\n",
       "      <td>271.50</td>\n",
       "      <td>210.75</td>\n",
       "      <td>216.75</td>\n",
       "      <td>17.789707</td>\n",
       "      <td>6.573437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>286.85</td>\n",
       "      <td>303.10</td>\n",
       "      <td>271.00</td>\n",
       "      <td>256.00</td>\n",
       "      <td>93.85</td>\n",
       "      <td>102.60</td>\n",
       "      <td>83.90</td>\n",
       "      <td>83.90</td>\n",
       "      <td>255.70</td>\n",
       "      <td>223.35</td>\n",
       "      <td>235.30</td>\n",
       "      <td>222.35</td>\n",
       "      <td>20.244898</td>\n",
       "      <td>6.446430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>262.60</td>\n",
       "      <td>276.90</td>\n",
       "      <td>253.10</td>\n",
       "      <td>249.15</td>\n",
       "      <td>80.70</td>\n",
       "      <td>80.95</td>\n",
       "      <td>95.15</td>\n",
       "      <td>105.15</td>\n",
       "      <td>167.30</td>\n",
       "      <td>151.35</td>\n",
       "      <td>153.90</td>\n",
       "      <td>189.45</td>\n",
       "      <td>22.572217</td>\n",
       "      <td>8.409815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>214.70</td>\n",
       "      <td>234.15</td>\n",
       "      <td>222.85</td>\n",
       "      <td>209.90</td>\n",
       "      <td>89.25</td>\n",
       "      <td>83.40</td>\n",
       "      <td>124.75</td>\n",
       "      <td>118.50</td>\n",
       "      <td>211.85</td>\n",
       "      <td>180.75</td>\n",
       "      <td>153.55</td>\n",
       "      <td>159.35</td>\n",
       "      <td>18.521585</td>\n",
       "      <td>7.482058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>301.65</td>\n",
       "      <td>296.00</td>\n",
       "      <td>287.00</td>\n",
       "      <td>311.80</td>\n",
       "      <td>87.45</td>\n",
       "      <td>95.90</td>\n",
       "      <td>110.70</td>\n",
       "      <td>101.35</td>\n",
       "      <td>183.45</td>\n",
       "      <td>236.10</td>\n",
       "      <td>212.75</td>\n",
       "      <td>235.25</td>\n",
       "      <td>23.918334</td>\n",
       "      <td>11.316462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>302.45</td>\n",
       "      <td>307.30</td>\n",
       "      <td>264.95</td>\n",
       "      <td>266.65</td>\n",
       "      <td>85.65</td>\n",
       "      <td>94.50</td>\n",
       "      <td>121.50</td>\n",
       "      <td>94.05</td>\n",
       "      <td>166.50</td>\n",
       "      <td>183.95</td>\n",
       "      <td>166.25</td>\n",
       "      <td>135.50</td>\n",
       "      <td>21.854913</td>\n",
       "      <td>10.691211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>308.35</td>\n",
       "      <td>312.10</td>\n",
       "      <td>247.25</td>\n",
       "      <td>256.20</td>\n",
       "      <td>81.70</td>\n",
       "      <td>84.60</td>\n",
       "      <td>92.25</td>\n",
       "      <td>92.25</td>\n",
       "      <td>232.55</td>\n",
       "      <td>218.00</td>\n",
       "      <td>203.65</td>\n",
       "      <td>200.90</td>\n",
       "      <td>15.794307</td>\n",
       "      <td>2.665436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>298.00</td>\n",
       "      <td>309.40</td>\n",
       "      <td>296.75</td>\n",
       "      <td>283.75</td>\n",
       "      <td>80.65</td>\n",
       "      <td>84.90</td>\n",
       "      <td>138.30</td>\n",
       "      <td>123.55</td>\n",
       "      <td>252.50</td>\n",
       "      <td>222.40</td>\n",
       "      <td>204.65</td>\n",
       "      <td>223.60</td>\n",
       "      <td>22.478367</td>\n",
       "      <td>7.553617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0       1       2       3       4       5       6       7       8   \\\n",
       "0   230.25  247.50  269.75  262.05   96.65  103.50  124.95  121.90  213.50   \n",
       "1   283.40  289.85  251.85  263.45   88.90   87.05  100.80   88.80  224.65   \n",
       "2   261.55  255.95  213.65  207.60   75.70   80.70  104.70   80.55  118.65   \n",
       "3   256.15  275.05  238.15  236.10   61.50   73.15  105.55   87.90  173.55   \n",
       "4   193.20  208.70  190.50  162.30   81.65   84.40   93.25   79.00  119.95   \n",
       "5   236.05  247.60  250.60  264.65   73.50   79.95  101.65  109.00  257.45   \n",
       "6   271.10  280.30  194.50  199.30   97.30  100.10  142.70  129.05  128.80   \n",
       "7   316.40  317.90  222.25  226.80   99.75  117.55  134.75  134.80  121.35   \n",
       "8   262.40  256.50  191.40  191.85   99.45  111.35  174.75  155.85  113.90   \n",
       "9   239.40  272.40  239.80  242.25   79.95   95.60  120.95  108.95  150.25   \n",
       "10  257.20  266.80  270.15  259.20  110.35  108.00  122.10  118.70  228.90   \n",
       "11  117.75  126.15  135.90  129.65   46.80   57.30   95.20   66.85   73.60   \n",
       "12  259.15  257.30  221.15  210.75   55.45   54.25   61.40   60.05  143.85   \n",
       "13  243.25  251.40  243.45  246.60   73.15   80.10  112.65  110.10  106.75   \n",
       "14  235.80  258.95  165.50  171.50   76.95   85.05   92.35  104.50  106.75   \n",
       "15  284.40  252.40  240.40  218.30   87.50   84.55   92.45  106.10  160.10   \n",
       "16  202.45  210.55  231.75  238.35   84.15   77.20   96.10   90.15  146.35   \n",
       "17  211.25  215.30  192.60  183.90   76.90   83.45   54.75   67.80  132.20   \n",
       "18  262.05  269.00  215.15  187.30   82.45   86.65  128.15  116.60  176.65   \n",
       "19  231.25  253.35  276.05  253.95   71.55   79.75   86.95   90.25  200.10   \n",
       "20  220.95  227.05  185.15  194.90   45.10   58.75  103.25  117.55  124.10   \n",
       "21  232.85  246.70  193.30  187.70   81.80   87.00  103.35   96.20  136.30   \n",
       "22  272.40  273.95  224.05  229.75   87.65   90.95   88.20   98.25  187.65   \n",
       "23  181.75  191.50  204.05  209.95   67.90   85.05   75.55   73.75   91.85   \n",
       "24  301.25  298.05  272.55  292.15   96.80  100.40  118.95  118.95  165.05   \n",
       "25  199.85  223.15  226.00  217.20   83.40   89.00  112.60   93.20  214.30   \n",
       "26  202.05  219.00  160.60  163.00   62.50   78.60   94.60   90.80  196.85   \n",
       "27  199.60  199.80  231.50  218.45   61.40   73.55   87.15   90.85  138.45   \n",
       "28  280.55  273.80  212.35  194.55   77.40   78.40   92.50   78.10  213.15   \n",
       "29  227.85  228.30  178.80  163.80   77.10   74.15  106.45  102.10  260.65   \n",
       "30  282.05  252.40  239.25  225.30   82.20   91.70   79.70   87.50  106.55   \n",
       "31  319.65  333.95  260.70  261.25   74.65   91.90  111.50  104.20  241.00   \n",
       "32  331.55  322.15  211.05  235.40   82.70   93.50  161.95  145.95  182.40   \n",
       "33  211.00  216.40  171.55  160.70   67.90   72.55   78.20   69.15  110.40   \n",
       "34  263.40  274.00  237.55  209.95   63.70   66.05   85.70   92.30  300.20   \n",
       "35  286.85  303.10  271.00  256.00   93.85  102.60   83.90   83.90  255.70   \n",
       "36  262.60  276.90  253.10  249.15   80.70   80.95   95.15  105.15  167.30   \n",
       "37  214.70  234.15  222.85  209.90   89.25   83.40  124.75  118.50  211.85   \n",
       "38  301.65  296.00  287.00  311.80   87.45   95.90  110.70  101.35  183.45   \n",
       "39  302.45  307.30  264.95  266.65   85.65   94.50  121.50   94.05  166.50   \n",
       "40  308.35  312.10  247.25  256.20   81.70   84.60   92.25   92.25  232.55   \n",
       "41  298.00  309.40  296.75  283.75   80.65   84.90  138.30  123.55  252.50   \n",
       "\n",
       "        9       10      11         12         13  \n",
       "0   175.65  184.60  208.25  22.119015   8.402430  \n",
       "1   274.90  156.85  162.50  22.959088   7.806316  \n",
       "2    97.90  177.80  155.50  20.443594  10.779055  \n",
       "3   190.45  156.30  148.60  18.710698   9.761427  \n",
       "4   107.55  108.45  126.30  19.066406  11.378716  \n",
       "5   250.35  193.70  186.45  21.171885   5.501083  \n",
       "6   137.40  176.35  177.35  18.339100   4.258123  \n",
       "7   222.70  237.55  219.20  23.323416  18.607747  \n",
       "8   151.70  148.35  119.65  18.903592  12.966332  \n",
       "9   154.80  185.70  176.15  20.588235   8.745554  \n",
       "10  261.30  203.70  173.35  21.110727   6.959002  \n",
       "11   91.30  134.25  134.40  17.977701  14.684780  \n",
       "12   89.25  138.65  132.05  20.746579   8.749294  \n",
       "13  135.20  122.10  135.05  21.303949   8.944358  \n",
       "14  116.45  124.95  124.10  17.716263   7.815455  \n",
       "15  177.20  152.35  157.85  19.571681   8.811978  \n",
       "16  176.75  143.45  132.70  19.694689   8.260906  \n",
       "17  183.70  172.80  199.25  22.392290  15.508384  \n",
       "18  168.45  185.65  176.85  20.402309   6.514257  \n",
       "19  145.05  172.50  167.45  21.714286  10.542856  \n",
       "20  174.80  122.75  109.95  20.237387  17.237435  \n",
       "21  117.75  161.00  154.80  19.759871   6.596818  \n",
       "22  131.85  141.60  172.55  20.807487  11.644363  \n",
       "23   69.40  155.80  168.55  21.679401  11.420302  \n",
       "24  159.30  222.25  213.80  22.862369   3.209735  \n",
       "25  167.70  177.80  166.95  20.826921  11.223981  \n",
       "26  171.25  127.10  117.35  19.136719  10.056046  \n",
       "27  178.70  119.80  115.90  21.852535  10.349745  \n",
       "28  197.85  230.90  232.80  19.930796   5.941474  \n",
       "29  202.15  180.00  171.70  21.250983   7.259064  \n",
       "30   73.90  153.15  140.95  20.485977  12.715976  \n",
       "31  237.15  202.55  185.35  21.420747   7.404796  \n",
       "32  204.95  197.55  218.25  22.589532  10.025477  \n",
       "33   90.10  136.60  132.05  18.749258   8.170617  \n",
       "34  271.50  210.75  216.75  17.789707   6.573437  \n",
       "35  223.35  235.30  222.35  20.244898   6.446430  \n",
       "36  151.35  153.90  189.45  22.572217   8.409815  \n",
       "37  180.75  153.55  159.35  18.521585   7.482058  \n",
       "38  236.10  212.75  235.25  23.918334  11.316462  \n",
       "39  183.95  166.25  135.50  21.854913  10.691211  \n",
       "40  218.00  203.65  200.90  15.794307   2.665436  \n",
       "41  222.40  204.65  223.60  22.478367   7.553617  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xi = Normalize(strategy='IMC').fit_transform(X_train)  # m, weight, height, IMC\n",
    "xi = TestSide(strategy='mean').fit_transform(X_train)  # mean, m\n",
    "xi = FeaturesAdder(new_features=['IMC', 'imbalance']).fit_transform(X_train)  # m, IMC, imbalance, height-weight\n",
    "\n",
    "\n",
    "pd.DataFrame(xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "preprocessing = make_pipeline(\n",
    "    Normalize(strategy='IMC'),\n",
    "    TestSide(strategy='mean'),\n",
    "    FeaturesAdder(new_features=['IMC', 'imbalance'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 6 is out of bounds for axis 1 with size 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-95cb7d290930>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/mvc/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlast_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlast_step\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mvc/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-434a7f870d7c>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mX_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_cols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'IMC'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mX_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_copy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_cols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'anthropo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 6 is out of bounds for axis 1 with size 6"
     ]
    }
   ],
   "source": [
    "preprocessing.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "full_pipeline = make_pipeline(\n",
    "    preprocessing,\n",
    "    XGBRegressor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "b'[09:53:27] src/objective/regression_obj.cc:89: Check failed: (info.labels.size()) != (0) label set cannot be empty'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-1f5b3dc437c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfull_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/mvc/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mvc/lib/python3.6/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, eval_set, eval_metric, early_stopping_rounds, verbose)\u001b[0m\n\u001b[1;32m    249\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                               verbose_eval=verbose)\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mvc/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, learning_rates, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mvc/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mvc/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m             \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mvc/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \"\"\"\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: b'[09:53:27] src/objective/regression_obj.cc:89: Check failed: (info.labels.size()) != (0) label set cannot be empty'"
     ]
    }
   ],
   "source": [
    "full_pipeline.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romain/miniconda3/envs/mvc/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiOutputRegressor(estimator=XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1),\n",
       "           n_jobs=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgboost_multi = MultiOutputRegressor(XGBRegressor())\n",
    "\n",
    "xgboost_multi.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize: height, weight, IMC, nothing\n",
    "# TestSide: mean, both, Fscore\n",
    "# FeatureAdder: IMC, imbalance, height-weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_pred_reg = xgboost_multi.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dyn\n",
      "rmse = 1.969\n",
      "mape = 12.985\n",
      "----------\n",
      "BodyBoost\n",
      "rmse = 0.834\n",
      "mape = 8.167\n",
      "----------\n",
      "MeanEggBeater\n",
      "rmse = 0.778\n",
      "mape = 8.669\n",
      "----------\n",
      "MaxEggBeater\n",
      "rmse = 1.110\n",
      "mape = 10.755\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "for i, output in enumerate(range(y_pred_reg.shape[1])):\n",
    "    print(y_description[i])\n",
    "    mse = mean_squared_error(y_test[:, i], y_pred_reg[:, i])\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f'rmse = {rmse:.3f}')\n",
    "    mape = mean_absolute_percentage_error(y_test[:, i], y_pred_reg[:, i])\n",
    "    print(f'mape = {mape:.3f}')\n",
    "    print('-' * 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mvc]",
   "language": "python",
   "name": "conda-env-mvc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
